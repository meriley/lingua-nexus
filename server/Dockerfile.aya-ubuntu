# Ubuntu-based Dockerfile for Aya GGUF support
# Resolves conda conflicts by using native Python installation
FROM ubuntu:20.04

# Set environment variables to avoid interactive prompts
ENV DEBIAN_FRONTEND=noninteractive
ENV TZ=UTC
ENV PYTHONUNBUFFERED=1

# Configure timezone first
RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone

# Install system dependencies including Python 3.10
RUN apt-get update && apt-get install -y \
    software-properties-common \
    wget \
    curl \
    git \
    && add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3.10-distutils \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install build dependencies for llama-cpp-python with CUDA support
RUN apt-get update && apt-get install -y \
    cmake \
    build-essential \
    gcc \
    g++ \
    pkg-config \
    libssl-dev \
    libffi-dev \
    libgomp1 \
    libopenblas-dev \
    libomp-dev \
    nvidia-cuda-toolkit \
    && rm -rf /var/lib/apt/lists/*

# Set Python 3.10 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1

# Install pip for Python 3.10
RUN curl https://bootstrap.pypa.io/get-pip.py | python3.10

# Upgrade pip and setuptools
RUN python3.10 -m pip install --upgrade pip setuptools wheel

# Set working directory
WORKDIR /app

# Create non-root user early
RUN useradd -m -u 1000 aya && chown -R aya:aya /app

# Copy requirements first (for better caching)
COPY requirements-aya.txt .

# Set environment for CUDA compilation
ENV CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS -DGGML_CUDA=ON"
ENV FORCE_CMAKE=1
ENV PIP_NO_CACHE_DIR=1
ENV CUDA_DOCKER_ARCH=all

# Install FastAPI and core dependencies first
RUN python3.10 -m pip install fastapi>=0.100.0 uvicorn>=0.22.0 pydantic>=1.10.7 python-dotenv>=1.0.0

# Install PyTorch CPU-only version (much smaller than CUDA version)
RUN python3.10 -m pip install "torch>=2.1.0,<2.2.0" --index-url https://download.pytorch.org/whl/cpu

# Install remaining dependencies except transformers (will install lighter version)
RUN python3.10 -m pip install \
    sentencepiece==0.1.99 \
    slowapi>=0.1.7 \
    python-multipart>=0.0.6 \
    typing_extensions>=4.4.0 \
    huggingface-hub>=0.19.0 \
    diskcache>=5.6.1

# Install transformers with compatible tokenizers version and numpy 1.x
RUN python3.10 -m pip install "transformers>=4.36.0,<4.41.0" --no-deps
RUN python3.10 -m pip install regex requests tqdm "numpy>=1.20.0,<2.0.0" packaging "tokenizers>=0.19,<0.20" safetensors>=0.4.1

# Compile and install llama-cpp-python with CUDA support
RUN python3.10 -m pip install llama-cpp-python --verbose

# Verify llama-cpp-python installation with CUDA support
RUN python3.10 -c "from llama_cpp import Llama; print('llama-cpp-python installed successfully')"
RUN python3.10 -c "import llama_cpp; print('CUDA available:', hasattr(llama_cpp.llama_cpp, 'GGML_USE_CUDA'))"

# Copy application code
COPY app/ ./app/
COPY *.py ./

# Set proper ownership
RUN chown -R aya:aya /app

# Set environment variables for runtime
ENV PYTHONPATH=/app
ENV HF_HOME=/app/.cache/huggingface
ENV MODEL_TYPE=aya
ENV ENABLE_MULTIMODEL=false
ENV USE_GGUF=true
ENV GGUF_FILENAME=aya-expanse-8b-Q4_K_M.gguf
ENV MODEL_PATH=bartowski/aya-expanse-8b-GGUF

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Create cache directory with proper permissions and switch to user
RUN mkdir -p /app/.cache/huggingface && chown -R aya:aya /app /app/.cache
USER aya

# Start the application with cache permission fix
CMD ["sh", "-c", "mkdir -p /app/.cache/huggingface && python3.10 -m uvicorn app.main_aya:app --host 0.0.0.0 --port 8000"]