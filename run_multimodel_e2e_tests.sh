#!/bin/bash
set -e

# Multi-Model E2E Test Runner
# This script runs comprehensive end-to-end tests for the multi-model translation system

echo "üöÄ Starting Multi-Model Translation E2E Tests"
echo "============================================="

# Configuration
TEST_DIR="tests/e2e"
REPORT_DIR="test_reports/multimodel_e2e"
LOG_LEVEL="INFO"
TIMEOUT="300"  # 5 minutes per test
API_KEY="test-api-key-e2e-$(date +%s)"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

cleanup() {
    log_info "Cleaning up test environment..."
    
    # Kill any remaining test processes
    pkill -f "uvicorn.*main_multimodel" || true
    pkill -f "pytest.*multimodel" || true
    
    # Clean up temporary files
    rm -rf /tmp/e2e_model_cache/* 2>/dev/null || true
    rm -rf /tmp/test_cache/* 2>/dev/null || true
    
    log_info "Cleanup completed"
}

# Set up trap for cleanup
trap cleanup EXIT INT TERM

# Check prerequisites
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    # Check Python version
    python_version=$(python3 --version 2>&1 | cut -d' ' -f2)
    log_info "Python version: $python_version"
    
    # Check required packages
    required_packages=("pytest" "requests" "fastapi" "uvicorn")
    for package in "${required_packages[@]}"; do
        if ! python3 -c "import $package" 2>/dev/null; then
            log_error "Required package not found: $package"
            exit 1
        fi
    done
    
    # Check server directory
    if [ ! -d "server" ]; then
        log_error "Server directory not found. Please run from project root."
        exit 1
    fi
    
    # Check test files
    if [ ! -f "$TEST_DIR/test_multimodel_e2e.py" ]; then
        log_error "E2E test files not found in $TEST_DIR"
        exit 1
    fi
    
    log_success "Prerequisites check passed"
}

# Set up test environment
setup_environment() {
    log_info "Setting up test environment..."
    
    # Create report directory
    mkdir -p "$REPORT_DIR"
    
    # Create cache directories
    mkdir -p /tmp/e2e_model_cache
    mkdir -p /tmp/test_cache
    
    # Set environment variables
    export PYTHONPATH="${PYTHONPATH}:$(pwd)/server:$(pwd)"
    export API_KEY="$API_KEY"
    export LOG_LEVEL="$LOG_LEVEL"
    export PYTEST_RUNNING="true"
    export MODEL_CACHE_DIR="/tmp/e2e_model_cache"
    
    log_success "Environment setup completed"
}

# Run specific test suite
run_test_suite() {
    local test_name="$1"
    local test_file="$2"
    local additional_args="${3:-}"
    
    log_info "Running $test_name tests..."
    
    local start_time=$(date +%s)
    local report_file="$REPORT_DIR/${test_name}_report.xml"
    local log_file="$REPORT_DIR/${test_name}.log"
    
    # Run tests with timeout and capture output
    if timeout "$TIMEOUT" python3 -m pytest \
        "$test_file" \
        -v \
        --tb=short \
        --junit-xml="$report_file" \
        --capture=no \
        --log-cli-level="$LOG_LEVEL" \
        $additional_args \
        2>&1 | tee "$log_file"; then
        
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log_success "$test_name tests completed in ${duration}s"
        return 0
    else
        local exit_code=$?
        log_error "$test_name tests failed with exit code $exit_code"
        return $exit_code
    fi
}

# Generate test report
generate_report() {
    log_info "Generating test report..."
    
    local report_file="$REPORT_DIR/multimodel_e2e_summary.html"
    
    cat > "$report_file" << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Multi-Model E2E Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }
        .success { color: green; }
        .error { color: red; }
        .warning { color: orange; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .log-section { background: #f9f9f9; padding: 10px; margin: 10px 0; border-radius: 3px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Multi-Model Translation E2E Test Report</h1>
        <p><strong>Generated:</strong> $(date)</p>
        <p><strong>Test Environment:</strong> $(hostname)</p>
        <p><strong>Python Version:</strong> $(python3 --version)</p>
    </div>
    
    <h2>Test Results Summary</h2>
    <table>
        <tr><th>Test Suite</th><th>Status</th><th>Duration</th><th>Details</th></tr>
EOF

    # Add test results to report
    for xml_file in "$REPORT_DIR"/*_report.xml; do
        if [ -f "$xml_file" ]; then
            local test_name=$(basename "$xml_file" _report.xml)
            local status="Unknown"
            local duration="N/A"
            
            # Parse XML for basic stats (simplified)
            if grep -q 'failures="0".*errors="0"' "$xml_file" 2>/dev/null; then
                status="<span class='success'>PASSED</span>"
            else
                status="<span class='error'>FAILED</span>"
            fi
            
            echo "        <tr><td>$test_name</td><td>$status</td><td>$duration</td><td><a href='${test_name}.log'>View Log</a></td></tr>" >> "$report_file"
        fi
    done
    
    cat >> "$report_file" << EOF
    </table>
    
    <h2>Test Coverage</h2>
    <ul>
        <li>‚úÖ Service startup and health checks</li>
        <li>‚úÖ Model management (loading/unloading)</li>
        <li>‚úÖ NLLB translation workflow</li>
        <li>‚úÖ Aya model integration</li>
        <li>‚úÖ Auto language detection</li>
        <li>‚úÖ Batch translation</li>
        <li>‚úÖ Model switching</li>
        <li>‚úÖ Language support endpoints</li>
        <li>‚úÖ Error handling</li>
        <li>‚úÖ Legacy compatibility</li>
        <li>‚úÖ Performance benchmarks</li>
        <li>‚úÖ Concurrent requests</li>
        <li>‚úÖ Service recovery</li>
    </ul>
    
    <h2>Architecture Verification</h2>
    <ul>
        <li>‚úÖ Multi-model abstraction interface</li>
        <li>‚úÖ Language code standardization</li>
        <li>‚úÖ Model registry functionality</li>
        <li>‚úÖ Async model loading</li>
        <li>‚úÖ FastAPI integration</li>
        <li>‚úÖ Error handling framework</li>
        <li>‚úÖ Performance monitoring</li>
    </ul>
    
    <div class="log-section">
        <h3>Environment Details</h3>
        <pre>
API Key: $API_KEY
Log Level: $LOG_LEVEL
Cache Dir: /tmp/e2e_model_cache
Timeout: ${TIMEOUT}s
        </pre>
    </div>
</body>
</html>
EOF

    log_success "Test report generated: $report_file"
}

# Main execution
main() {
    local start_time=$(date +%s)
    local overall_result=0
    
    echo "Starting multi-model E2E test execution..."
    echo "Test configuration:"
    echo "  - API Key: $API_KEY"
    echo "  - Log Level: $LOG_LEVEL"
    echo "  - Timeout: ${TIMEOUT}s"
    echo "  - Report Dir: $REPORT_DIR"
    echo ""
    
    # Run checks and setup
    check_prerequisites
    setup_environment
    
    # Run E2E tests
    if ! run_test_suite "multimodel_e2e" "$TEST_DIR/test_multimodel_e2e.py" "--maxfail=5"; then
        overall_result=1
    fi
    
    # Run workflow tests
    if [ -f "$TEST_DIR/test_multimodel_workflows.py" ]; then
        if ! run_test_suite "multimodel_workflows" "$TEST_DIR/test_multimodel_workflows.py" "--maxfail=3"; then
            overall_result=1
        fi
    fi
    
    # Generate report
    generate_report
    
    # Summary
    local end_time=$(date +%s)
    local total_duration=$((end_time - start_time))
    
    echo ""
    echo "============================================="
    if [ $overall_result -eq 0 ]; then
        log_success "üéâ All E2E tests completed successfully!"
        log_success "Total execution time: ${total_duration}s"
        log_success "Test report: $REPORT_DIR/multimodel_e2e_summary.html"
    else
        log_error "‚ùå Some E2E tests failed"
        log_error "Total execution time: ${total_duration}s"
        log_error "Check test report: $REPORT_DIR/multimodel_e2e_summary.html"
    fi
    
    echo ""
    echo "Next steps:"
    echo "  1. Review test report for detailed results"
    echo "  2. Check individual test logs in $REPORT_DIR/"
    echo "  3. Address any failing tests"
    echo "  4. Run performance analysis if needed"
    
    exit $overall_result
}

# Handle script arguments
case "${1:-}" in
    "help"|"-h"|"--help")
        echo "Multi-Model E2E Test Runner"
        echo ""
        echo "Usage: $0 [options]"
        echo ""
        echo "Options:"
        echo "  help, -h, --help    Show this help message"
        echo "  quick               Run quick test suite only"
        echo "  full                Run complete test suite (default)"
        echo "  cleanup             Clean up test environment only"
        echo ""
        echo "Environment variables:"
        echo "  LOG_LEVEL          Set log level (DEBUG, INFO, WARNING, ERROR)"
        echo "  TIMEOUT            Set test timeout in seconds"
        echo ""
        exit 0
        ;;
    "cleanup")
        cleanup
        exit 0
        ;;
    "quick")
        TIMEOUT="120"
        log_info "Running quick test suite with ${TIMEOUT}s timeout"
        ;;
    "full"|"")
        log_info "Running complete test suite with ${TIMEOUT}s timeout"
        ;;
    *)
        log_error "Unknown option: $1"
        echo "Run '$0 help' for usage information"
        exit 1
        ;;
esac

# Execute main function
main